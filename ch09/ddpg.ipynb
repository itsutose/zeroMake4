{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import random\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity=1e6):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque([], maxlen=int(capacity))\n",
    "\n",
    "    def append(self, *args):\n",
    "        transition = Transition(*args)\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def reset(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    def length(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def init_weight(size):\n",
    "    f = size[0]\n",
    "    v = 1. / np.sqrt(f)\n",
    "    return torch.tensor(np.random.uniform(low=-v, high=v, size=size), dtype=torch.float)\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, num_state, num_action, hidden1_size=400, hidden2_size=300, init_w=3e-3):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_state[0], hidden1_size)\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.fc3 = nn.Linear(hidden2_size, num_action[0])\n",
    "\n",
    "        self.num_state = num_state\n",
    "        self.num_action = num_action\n",
    "\n",
    "        self.fc1.weight.data = init_weight(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = init_weight(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        y = torch.tanh(self.fc3(h))  # 2をかけてもよいかも？\n",
    "        return y\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, num_state, num_action, hidden1_size=400, hidden2_size=300, init_w=3e-4):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_state[0], hidden1_size)\n",
    "        self.fc2 = nn.Linear(hidden1_size+num_action[0], hidden2_size)\n",
    "        self.fc3 = nn.Linear(hidden2_size, 1)\n",
    "\n",
    "        self.num_state = num_state\n",
    "        self.num_action = num_action\n",
    "\n",
    "        self.fc1.weight.data = init_weight(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = init_weight(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, x, action):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(torch.cat([h, action], dim=1)))\n",
    "        y = self.fc3(h)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class OrnsteinUhlenbeckProcess:\n",
    "    def __init__(self, theta=0.15, mu=0.0, sigma=0.2, dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.size = size\n",
    "        self.num_steps = 0\n",
    "\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)\n",
    "\n",
    "        if sigma_min is not None:\n",
    "            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma_min\n",
    "        else:\n",
    "            self.m = 0\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma\n",
    "\n",
    "    def current_sigma(self):\n",
    "        sigma = max(self.sigma_min, self.m * float(self.num_steps) + self.c)\n",
    "        return sigma\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma() * np.sqrt(self.dt) * np.random.normal(size=self.size)\n",
    "        self.x_prev = x\n",
    "        self.num_steps += 1\n",
    "        return x\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    def __init__(self, actor, critic, optimizer_actor, optimizer_critic, replay_buffer, device, gamma=0.99, tau=1e-3, epsilon=1.0, batch_size=64):\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.optimizer_actor = optimizer_actor\n",
    "        self.optimizer_critic = optimizer_critic\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.random_process = OrnsteinUhlenbeckProcess(size=actor.num_action[0])\n",
    "\n",
    "        self.num_state = actor.num_state\n",
    "        self.num_action = actor.num_action\n",
    "\n",
    "    def add_memory(self, *args):\n",
    "        self.replay_buffer.append(*args)\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.replay_buffer.reset()\n",
    "\n",
    "    def get_action(self, state, greedy=False):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float, device=self.device).view(-1, *self.num_state)\n",
    "        action = self.actor(state_tensor)\n",
    "        if not greedy:\n",
    "            action += self.epsilon*torch.tensor(self.random_process.sample(), dtype=torch.float, device=self.device)\n",
    "\n",
    "        return action.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        state_batch = torch.tensor(batch.state, device=self.device, dtype=torch.float)\n",
    "        action_batch = torch.tensor(batch.action, device=self.device, dtype=torch.float)\n",
    "        next_state_batch = torch.tensor(batch.next_state, device=self.device, dtype=torch.float)\n",
    "        reward_batch = torch.tensor(batch.reward, device=self.device, dtype=torch.float).unsqueeze(1)\n",
    "        not_done = np.array([(not done) for done in batch.done])\n",
    "        not_done_batch = torch.tensor(not_done, device=self.device, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # need to change\n",
    "        qvalue = self.critic(state_batch, action_batch)\n",
    "        next_qvalue = self.critic_target(next_state_batch, self.actor_target(next_state_batch))\n",
    "        target_qvalue = reward_batch + (self.gamma * next_qvalue * not_done_batch) \n",
    "\n",
    "        critic_loss = F.mse_loss(qvalue, target_qvalue)\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "\n",
    "        actor_loss = -self.critic(state_batch, self.actor(state_batch)).mean()\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "\n",
    "        # soft parameter update\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yamaguchi\\AppData\\Local\\Temp\\ipykernel_43508\\944033600.py:64: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float, device=self.device).view(-1, *self.num_state)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 1 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m total_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_steps):\n\u001b[1;32m---> 35\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mget_action(observation)\n\u001b[0;32m     36\u001b[0m     next_observation, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     37\u001b[0m     total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[3], line 64\u001b[0m, in \u001b[0;36mDDPG.get_action\u001b[1;34m(self, state, greedy)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_action\u001b[39m(\u001b[39mself\u001b[39m, state, greedy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 64\u001b[0m     state_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(state, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat, device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_state)\n\u001b[0;32m     65\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor(state_tensor)\n\u001b[0;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m greedy:\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 3 at dim 1 (got 0)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネルがクラッシュしました。エラーの原因を特定するには、セル内のコードを確認してください。詳細については、<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a> をクリックしてください。さらなる詳細については、Jupyter [log] (command:jupyter.viewOutput) を参照してください。"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "max_episodes = 300\n",
    "memory_capacity = 1e6  # バッファの容量\n",
    "gamma = 0.99  # 割引率\n",
    "tau = 1e-3  # ターゲットの更新率\n",
    "epsilon = 1.0  # ノイズの量をいじりたい場合、多分いらない\n",
    "batch_size = 64\n",
    "lr_actor = 1e-4\n",
    "lr_critic = 1e-3\n",
    "logger_interval = 10\n",
    "weight_decay = 1e-2\n",
    "\n",
    "env = gym.make('Pendulum-v1', render_mode = 'human')\n",
    "num_state = env.observation_space.shape\n",
    "num_action = env.action_space.shape\n",
    "max_steps = env.spec.max_episode_steps\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "actorNet = ActorNetwork(num_state, num_action).to(device)\n",
    "criticNet = CriticNetwork(num_state, num_action).to(device)\n",
    "optimizer_actor = optim.Adam(actorNet.parameters(), lr=lr_actor)\n",
    "optimizer_critic = optim.Adam(criticNet.parameters(), lr=lr_critic, weight_decay=weight_decay)\n",
    "replay_buffer = ReplayBuffer(capacity=memory_capacity)\n",
    "agent = DDPG(actorNet, criticNet, optimizer_actor, optimizer_critic, replay_buffer, device, gamma, tau, epsilon, batch_size)\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    observation = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = agent.get_action(observation)\n",
    "        next_observation, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        agent.add_memory(observation, action, next_observation, reward, done)\n",
    "\n",
    "        agent.train()\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if episode % logger_interval == 0:\n",
    "        print(\"episode:{} total reward:{}\".format(episode, total_reward))\n",
    "\n",
    "for episode in range(3):\n",
    "    observation = env.reset()\n",
    "    env.render()\n",
    "    for step in range(max_steps):\n",
    "        action = agent.get_action(observation, greedy=True)\n",
    "        next_observation, reward, done, _ = env.step(action)\n",
    "        observation = next_observation\n",
    "        env.render()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
